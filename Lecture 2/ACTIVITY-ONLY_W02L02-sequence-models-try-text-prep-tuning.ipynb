{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69383602-32d0-44b4-86d5-a29d46674555",
   "metadata": {},
   "source": [
    "# AML Week 2, Lecture 2: Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e37a1-8e2d-49f0-9652-39896926fd19",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc57c4f-3b2c-4f6f-86aa-68f474a45656",
   "metadata": {},
   "source": [
    "- How to build, fit, and evaluat sequence models (SimpleRNN, LSTM, GRU, Bidrectional)\n",
    "- How to use Global Average Pooling + return_sequences=True\n",
    "- How to use pre-trained word embeddings for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7e2ff-dcf2-4b44-addc-6215aa8a9223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Then Set Random Seeds\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# Then run the Enable Deterministic Operations Function\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# MacOS Sonoma Fix\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3487b0-db87-4f70-bc6c-a48e62956740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding parent directory to python path\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8d776-63e5-4a02-afd8-04628e4c858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the autoreload extension\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import custom_functions_SOLUTION  as fn\n",
    "# fn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90fe76e-5fa7-436e-9634-d67e5b470eab",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b5806-1bf5-4c55-8e39-3f9159e26d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open(\"../Data-AmazonReviews/Amazon Product Reviews.md\") as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89879753-84c6-40cb-a2a8-8d6e7221cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "\n",
    "# Define a function for building an LSTM model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, optimizers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b67cf1a-97b6-4d3b-b103-694e4a6c6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "df = joblib.load('../Data-AmazonReviews/processed_data.joblib')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88999b3f-1b93-49ee-950b-9f0bdb426be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(x):\n",
    "    if x>=5.0:\n",
    "        return \"high\"\n",
    "    elif x <=2.0:\n",
    "        return \"low\"\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f5e92-f195-4ded-ac66-934ecb6644a2",
   "metadata": {},
   "source": [
    "To understand what customers do and do not like about Hoover products, we will define 2 groups:\n",
    "- High Ratings\n",
    "    - Overall rating = 5.0\n",
    "- Low Ratings\n",
    "    - Overall rating = 1.0 or 2.0\n",
    "\n",
    "\n",
    "We can use a function and .map to define group names based on the numeric overall ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda1d0b-e442-4f09-b334-c3397ab11834",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the function to create a new \"rating\" column with groups\n",
    "df['rating'] = df['overall'].map(create_groups)\n",
    "df['rating'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caeeaad-6db4-4dbf-a9e2-fcd3dfd30334",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check class balance of 'rating'\n",
    "df['rating'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9010c-9582-42e3-b958-507c65ad08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df_ml without null ratings\n",
    "df_ml = df.dropna(subset=['rating']).copy()\n",
    "df_ml.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd85e99-a07d-4475-9d20-134e1695c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "## X - Option A)  lemmas\n",
    "# def join_tokens(token_list):\n",
    "#     joined_tokens = ' '.join(token_list)\n",
    "#     return joined_tokens\n",
    "# X = df_ml['spacy_lemmas'].apply(join_tokens)\n",
    "\n",
    "# X - Option B) original raw text\n",
    "X = df_ml['text']\n",
    "\n",
    "# y - use our binary target \n",
    "y = df_ml['rating']\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea038ef0-08da-4dc5-b7b3-41f4652ca28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486c413-1286-46df-b939-b6cc423e9b4e",
   "metadata": {},
   "source": [
    "## From Train-Test Split for ML to Train-Test-Val Split for ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd218e-3911-49e7-9776-c07dc9087ae2",
   "metadata": {},
   "source": [
    "- Starting with a simple train-test-split for ML model (like in movie nlp project)\n",
    "- Resampling Imbalanced training data\n",
    "- Creating tensorflow dataset from X_train, y_train (so dataset is rebalanced)\n",
    "- Creating tensorflow dataset (intended to be split in 2 ) for X_test and y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c6b64-a923-4bcd-a899-8d1c43181cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 70:30 train test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "len(X_train_full), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5dc81c-53c6-486c-9beb-1ba5a86f6909",
   "metadata": {},
   "source": [
    "### Using Sklearn's LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f3486-8db8-4486-b931-bdc11d14a662",
   "metadata": {},
   "source": [
    "- Can't use text labels with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f098d599-e5a0-4d07-b8aa-a61ae8659921",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_full[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4195576-f430-446b-853a-da86d2bf0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instansiate label encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training target\n",
    "y_train_full_enc = encoder.fit_transform(y_train_full)#.values)\n",
    "\n",
    "# Fit and tranform the test target\n",
    "y_test_enc = encoder.transform(y_test)\n",
    "\n",
    "y_train_full_enc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b8926d-f87c-40dd-b883-6ab44f054b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Class names saved as .classes_\n",
    "classes = encoder.classes_\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552b075-6d94-4ad7-8c05-025d60ac00c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can inverse-transform \n",
    "encoder.inverse_transform([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cb1dc-d13d-4dcb-80ca-b8301f0f3581",
   "metadata": {},
   "source": [
    "### Undersampling Majority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d98790-5804-4162-9e78-4b0b3ff8faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Instantiate a RandomUnderSampler\n",
    "sampler = RandomUnderSampler(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69b21c-e59a-44f2-9619-ec706e571518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit_resample on the reshaped X_train data and y-train data\n",
    "X_train, y_train_enc = sampler.fit_resample(X_train_full.values.reshape(-1,1),\n",
    "                                        y_train_full_enc)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6157b-9f31-48fb-80d0-db4c35f5fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the reshaped X_train data back to 1D\n",
    "X_train = X_train.flatten()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898e60d-cb19-408e-91d4-e677f033a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class balance\n",
    "pd.Series(y_train_enc).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9880fe72-4a80-4135-ba02-1c2a5f873a21",
   "metadata": {},
   "source": [
    "## Previous Class' ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319c2f9-60a7-4f4e-b2e2-acf0a1fe0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b488903-2846-4bfc-a046-41ebb7ccebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a model pipeline \n",
    "count_pipe = Pipeline([('vectorizer',  CountVectorizer()), \n",
    "                       ('naivebayes',  MultinomialNB())])\n",
    "\n",
    "count_pipe.fit(X_train, y_train_enc)\n",
    "fn.evaluate_classification(count_pipe, X_train, y_train_enc, X_test, y_test_enc,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e80644-9fda-416e-80b6-789bb7a5903d",
   "metadata": {},
   "source": [
    "## Preparing For Deep NLP (Train-Test-Val Datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da20ab9e-7e58-445b-a0bf-38f9926e8d56",
   "metadata": {},
   "source": [
    "Since we already have train/test X and y vars, we will make 2 dataset objects using tf.data.Dataset.from_tensor_slices.\n",
    "\n",
    "1. The training dataset using X_train, y_train (that we resampled/balanced)\n",
    "2. The val/test dataset using X_test, y-test.\n",
    "\n",
    "We will then split the val/test dataset into a val/test split.\n",
    "\n",
    "<!-- \n",
    "### T/T/V Split - Order of Operations (if using 1 dataset object)\n",
    "\n",
    "1) **Create full dataset object & Shuffle Once.**\n",
    "2) Calculate number of samples for training and validation data.\n",
    "3) Create the train/test/val splits using .take() and .skip()\n",
    "4) **Add shuffle to the train dataset only.**\n",
    "5) (Optional/Not Used on LP) If applying a transformation (e.g. train_ds.map(...)) to the data, add  here, before .cache()\n",
    "7) (Optional) Add .cache() to all splits to increase speed  (but may cause problems with large datasets)\n",
    "8) **Add .batch to all splits (default batch size=32)**\n",
    "9) (Optional) Add .prefetch(tf.data.AUTOTUNE)\n",
    "10) (Optional) Print out final length of datasets -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0658b2a-2c35-4f27-a06e-a54416aa9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to Dataset Object\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train_enc))\n",
    "# Shuffle dataset once\n",
    "train_ds = train_ds.shuffle(len(train_ds),seed=42, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedeaafa-fa13-4309-a8a8-42913843b8c3",
   "metadata": {},
   "source": [
    "Create a test and validation dataset using X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e890160-24b8-4bcf-a3af-5d88bd0237a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test to dataset object to split\n",
    "val_test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69060a8-96f1-4398-9242-af1b3701b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate # of samples for 50/50 val/test split\n",
    "n_val_samples = int(len(val_test_ds) *.5)\n",
    "n_val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495b3b9-b89e-41c8-b597-d65a383cddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform the val/test split\n",
    "\n",
    "\n",
    "## Create the validation dataset using .take\n",
    "val_ds = val_test_ds.take(n_val_samples)\n",
    "\n",
    "## Create the test dataset using skip\n",
    "test_ds = val_test_ds.skip(n_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4a489-7074-49ea-be83-ea6c6f54cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the len gths of all 3 splits\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6305413-cb77-4238-9be5-f8ea32e7a9ee",
   "metadata": {},
   "source": [
    "### Adding Shuffling and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66afce4b-e39e-458b-b022-ac39ed9ca9ef",
   "metadata": {},
   "source": [
    "Let's examine a single element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4678d3f-baba-4962-87cf-00910be748f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a sample single element \n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X,'\\n\\n',example_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b9ebe2-9d41-4641-8961-88c3dfa7adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Repeat) display a sample single element \n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X,'\\n\\n',example_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27c52db3-92d2-4df6-822e-ee53198a20d8",
   "metadata": {},
   "source": [
    "Notice that we have the same example, the training data is not shuffling.\n",
    "\n",
    "Add .shuffle the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4969390-4e72-4e52-9fe7-41f83e683729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle only the training data every epoch\n",
    "train_ds = train_ds.shuffle(len(train_ds), seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8b84c-a4b2-464e-a7ef-21e2891df7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Repeat) display a sample single element \n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X,'\\n\\n',example_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8caedf-696c-4284-a224-194e64319af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Repeat) display a sample single element \n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X,'\\n\\n',example_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99b4bd-6d3c-409e-956b-a7d2e0fd5916",
   "metadata": {},
   "source": [
    "> Add batching (use 32 for batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48eeddb-5554-4f37-a435-91ea27911e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Setting the batch_size for all datasets\n",
    "BATCH_SIZE =32\n",
    "# use .batch to add batching to all 3 datasets\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "val_ds = val_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Confirm the number of batches in each\n",
    "print (f' There are {len(train_ds)} training batches.')\n",
    "print (f' There are {len(val_ds)} validation batches.')\n",
    "print (f' There are {len(test_ds)} testing batches.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498f8eb-bdaa-45bb-a809-f36014e03f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Repeat) display a sample single element \n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X,'\\n\\n',example_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48f3a6-6cb5-4065-9c49-4508e1f390c4",
   "metadata": {},
   "source": [
    "A single element now contains 32 samples since we set  batch_size to 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401b00d-d9b2-416c-b171-2e3cace3dd92",
   "metadata": {},
   "source": [
    "### Create the Training Texts Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45da873-5e7a-4e92-b4ca-939ad5c0e1f2",
   "metadata": {},
   "source": [
    "> We need to get a version of our data that is **only the texts**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a346be-2bf5-483d-8c13-4074fb506730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the text_ds from ds_train\n",
    "text_ds = train_ds.map(lambda x,y: x)\n",
    "# Preview the text_ds\n",
    "text_ds.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f4989-a040-4730-adfe-87321b0313e2",
   "metadata": {},
   "source": [
    "### Determine appropriate sequence length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fbbd2-718b-4766-912a-9fef9aa862ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ml['length (characters)'] = df_ml['text'].map(len)\n",
    "# df_ml.head(3)\n",
    "\n",
    "# ax = sns.histplot(data=df_ml, hue='rating', x='length (characters)',\n",
    "#                 stat='percent',common_norm=False)#, estimator='median',);\n",
    "# ax.axvline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9713fe-43cb-4bb9-aebc-e31601c543db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the length of the each text\n",
    "# We will split on each space, and then get the length\n",
    "df_ml['length (tokens)'] = df_ml['text'].map( lambda x: len(x.split(\" \")))\n",
    "df_ml['length (tokens)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473f73c-0ac0-461e-a186-bf45ad560e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 150\n",
    "ax = sns.histplot(data=df_ml, hue='rating', x='length (tokens)',kde=True,\n",
    "                stat='probability',common_norm=False)#, estimator='median',);\n",
    "ax.axvline(SEQUENCE_LENGTH, color='red', ls=\":\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1a448-8cc1-4bac-8896-0d278ebcfc5f",
   "metadata": {},
   "source": [
    "# Our First Deep Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95210b15-3546-48cb-9e2e-85669af45e6d",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46088607-41ee-49b1-93a4-d2405e591730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Create text Vectorization layer\n",
    "# SEQUENCE_LENGTH = 150\n",
    "EMBED_DIM = 100\n",
    "\n",
    "sequence_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "sequence_vectorizer.adapt(text_ds)\n",
    "VOCAB_SIZE = sequence_vectorizer.vocabulary_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873c9d2-f3f2-4b75-9230-c4caef9ebb75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "rnn_model = Sequential([\n",
    "    sequence_vectorizer,\n",
    "    layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                              output_dim=EMBED_DIM, \n",
    "                              input_length=SEQUENCE_LENGTH)\n",
    "    ])\n",
    "\n",
    "# Add *new* LSTM layer\n",
    "rnn_model.add(layers.SimpleRNN(32))\n",
    "\n",
    "# Add output layer\n",
    "rnn_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model.compile(optimizer='adam',#optimizers.legacy.Adam(learning_rate = .001), \n",
    "              loss='bce',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7e41c-89c9-4452-af8c-0888d4e605a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_callbacks(patience=5,#3,\n",
    "                  monitor='val_accuracy',\n",
    "                 restore_best_weights=False):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor,\n",
    "                                                 restore_best_weights=restore_best_weights)\n",
    "    return [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a31be-7344-4fc6-ab27-71439a895540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = rnn_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd54783-0891-4da0-aff9-0dbfd0cb3aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    rnn_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd497e6f-a936-46c8-a175-920a5d729f98",
   "metadata": {},
   "source": [
    "> We will continue with this task and introduce and apply various sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b37c47-bb28-4501-a7a1-5b7d95b63001",
   "metadata": {},
   "source": [
    "# 🕹️ Now Your Turn: Train A Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d786d06-e7a4-47f7-b68b-906260572960",
   "metadata": {},
   "source": [
    "> In a breakout room:\n",
    "    > - **Add the missing sequence layer(s) to the model below.**\n",
    "    > - Apply some of the additonal techniques we've shown thus far. \n",
    "    > - **Discuss you your breakout room group and decide which model to try.**\n",
    "    > - For example, try differnet sequence layers, try different values for dropout, number of units, using a GlobalAveragePooling layer or not, using a bidirectional layer or not, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52354e53-9d25-4a72-928a-0e1011c28020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_your_model(text_vectorization_layer):\n",
    "    \n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    #### Add your sequence layer and whatever additonal parameters/techniques you want to try\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96718d1-c4de-4af2-9c7d-34b396d9ed35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Build the lstm model and specify the vectorizer\n",
    "your_rnn_model = build_your_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = your_rnn_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16045a49-b75f-4e94-ba67-46e1d6ac177b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    your_rnn_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12dbbd9-60d5-4e74-a4cd-8acf0bff3890",
   "metadata": {},
   "source": [
    "# 🕹️(Optional) Your Turn: Try Alternative Text Preprocessing Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d54925-a7a3-400f-98f2-68062b6709de",
   "metadata": {},
   "source": [
    "Everything we have tuned/tested thus far has been related to the model itself. There are still several aspects of the text vectorization that we could also try:\n",
    "- EMBED_DIM (50,100,200,etc. )\n",
    "- SEQUENCE_LENGTH (50,100,150)\n",
    "- standardization (text vectorizer)\n",
    "- ngrams (text vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ae4b0-6fd0-4b2f-bf08-e33108229ae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create text Vectorization layer\n",
    "SEQUENCE_LENGTH = None\n",
    "EMBED_DIM = None\n",
    "\n",
    "sequence_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=None\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "sequence_vectorizer.adapt(text_ds)\n",
    "VOCAB_SIZE = sequence_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a73ec7-be39-454c-882c-9b79e2ed1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_your_model(text_vectorization_layer):\n",
    "    \n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "      \n",
    "    #### Add your sequence layer(s) and whatever additonal parameters/techniques you want to try\n",
    "\n",
    "    \n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_callbacks(patience=5,\n",
    "                  monitor='val_accuracy',\n",
    "                  start_from_epoch=0,\n",
    "                 restore_best_weights=False):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor, start_from_epoch=start_from_epoch,\n",
    "                                                 restore_best_weights=restore_best_weights)\n",
    "    return [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff158f-216c-4694-b2de-64e488913eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "your_model = build_your_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = your_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a783c-8a02-462b-b568-33ed6fae52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    your_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be2ddf1-f799-42df-ace3-e0dfaa395b18",
   "metadata": {},
   "source": [
    "> **Did you find any text preprocessing options that significantly improved the model?** If so, share with the class in the main zoom room or on Discord!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
