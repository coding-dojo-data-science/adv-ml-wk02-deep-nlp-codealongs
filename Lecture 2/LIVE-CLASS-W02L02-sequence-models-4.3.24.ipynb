{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69383602-32d0-44b4-86d5-a29d46674555",
   "metadata": {},
   "source": [
    "# AML Week 2, Lecture 2: Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e37a1-8e2d-49f0-9652-39896926fd19",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc57c4f-3b2c-4f6f-86aa-68f474a45656",
   "metadata": {},
   "source": [
    "- How to build, fit, and evaluat sequence models (SimpleRNN, LSTM, GRU, Bidrectional)\n",
    "- How to use Global Average Pooling + return_sequences=True\n",
    "- How to use pre-trained word embeddings for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7e2ff-dcf2-4b44-addc-6215aa8a9223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Then Set Random Seeds\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# Then run the Enable Deterministic Operations Function\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# MacOS Sonoma Fix\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3487b0-db87-4f70-bc6c-a48e62956740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding parent directory to python path\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8d776-63e5-4a02-afd8-04628e4c858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the autoreload extension\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import custom_functions_SOLUTION  as fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90fe76e-5fa7-436e-9634-d67e5b470eab",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b5806-1bf5-4c55-8e39-3f9159e26d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open(\"../Data-AmazonReviews/Amazon Product Reviews.md\") as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89879753-84c6-40cb-a2a8-8d6e7221cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n",
    "pd.set_option('display.max_colwidth', 250)\n",
    "\n",
    "# Define a function for building an LSTM model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, optimizers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b67cf1a-97b6-4d3b-b103-694e4a6c6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "df = joblib.load('../Data-AmazonReviews/processed_data.joblib')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88999b3f-1b93-49ee-950b-9f0bdb426be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(x):\n",
    "    if x>=5.0:\n",
    "        return \"high\"\n",
    "    elif x <=2.0:\n",
    "        return \"low\"\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f5e92-f195-4ded-ac66-934ecb6644a2",
   "metadata": {},
   "source": [
    "To understand what customers do and do not like about Hoover products, we will define 2 groups:\n",
    "- High Ratings\n",
    "    - Overall rating = 5.0\n",
    "- Low Ratings\n",
    "    - Overall rating = 1.0 or 2.0\n",
    "\n",
    "\n",
    "We can use a function and .map to define group names based on the numeric overall ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda1d0b-e442-4f09-b334-c3397ab11834",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the function to create a new \"rating\" column with groups\n",
    "df['rating'] = df['overall'].map(create_groups)\n",
    "df['rating'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caeeaad-6db4-4dbf-a9e2-fcd3dfd30334",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check class balance of 'rating'\n",
    "df['rating'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9010c-9582-42e3-b958-507c65ad08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df_ml without null ratings\n",
    "df_ml = df.dropna(subset=['rating']).copy()\n",
    "df_ml.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd85e99-a07d-4475-9d20-134e1695c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "## X - Option A)  lemmas\n",
    "# def join_tokens(token_list):\n",
    "#     joined_tokens = ' '.join(token_list)\n",
    "#     return joined_tokens\n",
    "# X = df_ml['spacy_lemmas'].apply(join_tokens)\n",
    "\n",
    "# X - Option B) original raw text\n",
    "X = df_ml['text']\n",
    "\n",
    "# y - use our binary target \n",
    "y = df_ml['rating']\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea038ef0-08da-4dc5-b7b3-41f4652ca28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486c413-1286-46df-b939-b6cc423e9b4e",
   "metadata": {},
   "source": [
    "## From Train-Test Split for ML to Train-Test-Val Split for ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd218e-3911-49e7-9776-c07dc9087ae2",
   "metadata": {},
   "source": [
    "- Starting with a simple train-test-split for ML model (like in movie nlp project)\n",
    "- Resampling Imbalanced training data\n",
    "- Creating tensorflow dataset from X_train, y_train (so dataset is rebalanced)\n",
    "- Creating tensorflow dataset (intended to be split in 2 ) for X_test and y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c6b64-a923-4bcd-a899-8d1c43181cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 70:30 train test split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "len(X_train_full), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5dc81c-53c6-486c-9beb-1ba5a86f6909",
   "metadata": {},
   "source": [
    "### Using Sklearn's LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f3486-8db8-4486-b931-bdc11d14a662",
   "metadata": {},
   "source": [
    "- Can't use text labels with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f098d599-e5a0-4d07-b8aa-a61ae8659921",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_full[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4195576-f430-446b-853a-da86d2bf0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instansiate label encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training target\n",
    "y_train_full_enc = encoder.fit_transform(y_train_full)#.values)\n",
    "\n",
    "# Fit and tranform the test target\n",
    "y_test_enc = encoder.transform(y_test)\n",
    "\n",
    "y_train_full_enc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b8926d-f87c-40dd-b883-6ab44f054b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Class names saved as .classes_\n",
    "classes = encoder.classes_\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552b075-6d94-4ad7-8c05-025d60ac00c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can inverse-transform \n",
    "encoder.inverse_transform([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cb1dc-d13d-4dcb-80ca-b8301f0f3581",
   "metadata": {},
   "source": [
    "### Undersampling Majority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d98790-5804-4162-9e78-4b0b3ff8faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Instantiate a RandomUnderSampler\n",
    "sampler = RandomUnderSampler(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69b21c-e59a-44f2-9619-ec706e571518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit_resample on the reshaped X_train data and y-train data\n",
    "X_train, y_train_enc = sampler.fit_resample(X_train_full.values.reshape(-1,1),\n",
    "                                        y_train_full_enc)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6157b-9f31-48fb-80d0-db4c35f5fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the reshaped X_train data back to 1D\n",
    "X_train = X_train.flatten()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898e60d-cb19-408e-91d4-e677f033a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class balance\n",
    "pd.Series(y_train_enc).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9880fe72-4a80-4135-ba02-1c2a5f873a21",
   "metadata": {},
   "source": [
    "## Last Week's ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319c2f9-60a7-4f4e-b2e2-acf0a1fe0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b488903-2846-4bfc-a046-41ebb7ccebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create a model pipeline \n",
    "# count_pipe = Pipeline([('vectorizer',  CountVectorizer()), \n",
    "#                        ('naivebayes',  MultinomialNB())])\n",
    "\n",
    "# count_pipe.fit(X_train, y_train_enc)\n",
    "# fn.evaluate_classification(count_pipe, X_train, y_train_enc, X_test, y_test_enc,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e80644-9fda-416e-80b6-789bb7a5903d",
   "metadata": {},
   "source": [
    "## Preparing For Deep NLP (Train-Test-Val Datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da20ab9e-7e58-445b-a0bf-38f9926e8d56",
   "metadata": {},
   "source": [
    "Since we already have train/test X and y vars, we will make 2 dataset objects using tf.data.Dataset.from_tensor_slices.\n",
    "\n",
    "1. The training dataset using X_train, y_train (that we resampled/balanced)\n",
    "2. The val/test dataset using X_test, y-test.\n",
    "\n",
    "We will then split the val/test dataset into a val/test split.\n",
    "\n",
    "<!-- \n",
    "### T/T/V Split - Order of Operations (if using 1 dataset object)\n",
    "\n",
    "1) **Create full dataset object & Shuffle Once.**\n",
    "2) Calculate number of samples for training and validation data.\n",
    "3) Create the train/test/val splits using .take() and .skip()\n",
    "4) **Add shuffle to the train dataset only.**\n",
    "5) (Optional/Not Used on LP) If applying a transformation (e.g. train_ds.map(...)) to the data, add  here, before .cache()\n",
    "7) (Optional) Add .cache() to all splits to increase speed  (but may cause problems with large datasets)\n",
    "8) **Add .batch to all splits (default batch size=32)**\n",
    "9) (Optional) Add .prefetch(tf.data.AUTOTUNE)\n",
    "10) (Optional) Print out final length of datasets -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0658b2a-2c35-4f27-a06e-a54416aa9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to Dataset Object\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train_enc))\n",
    "# Shuffle dataset once\n",
    "train_ds = train_ds.shuffle(len(train_ds),seed=42, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedeaafa-fa13-4309-a8a8-42913843b8c3",
   "metadata": {},
   "source": [
    "Create a test and validation dataset using X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e890160-24b8-4bcf-a3af-5d88bd0237a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test to dataset object to split\n",
    "val_test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69060a8-96f1-4398-9242-af1b3701b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate # of samples for 50/50 val/test split\n",
    "n_val_samples = int(len(val_test_ds) *.5)\n",
    "n_val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495b3b9-b89e-41c8-b597-d65a383cddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform the val/test split\n",
    "\n",
    "\n",
    "## Create the validation dataset using .take\n",
    "val_ds = val_test_ds.take(n_val_samples)\n",
    "\n",
    "## Create the test dataset using skip\n",
    "test_ds = val_test_ds.skip(n_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4a489-7074-49ea-be83-ea6c6f54cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the len gths of all 3 splits\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6305413-cb77-4238-9be5-f8ea32e7a9ee",
   "metadata": {},
   "source": [
    "### Adding Shuffling and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66afce4b-e39e-458b-b022-ac39ed9ca9ef",
   "metadata": {},
   "source": [
    "Let's examine a single element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4678d3f-baba-4962-87cf-00910be748f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a sample single element \n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X,'\\n\\n',example_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b9ebe2-9d41-4641-8961-88c3dfa7adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Repeat) display a sample single element \n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X,'\\n\\n',example_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27c52db3-92d2-4df6-822e-ee53198a20d8",
   "metadata": {},
   "source": [
    "Notice that we have the same example, the training data is not shuffling.\n",
    "\n",
    "Add .shuffle the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4969390-4e72-4e52-9fe7-41f83e683729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle only the training data every epoch\n",
    "train_ds = train_ds.shuffle(len(train_ds), seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8b84c-a4b2-464e-a7ef-21e2891df7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Repeat) display a sample single element \n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X,'\\n\\n',example_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8caedf-696c-4284-a224-194e64319af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Repeat) display a sample single element \n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X,'\\n\\n',example_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99b4bd-6d3c-409e-956b-a7d2e0fd5916",
   "metadata": {},
   "source": [
    "> Add batching (use 32 for batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48eeddb-5554-4f37-a435-91ea27911e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Setting the batch_size for all datasets\n",
    "BATCH_SIZE =32\n",
    "# use .batch to add batching to all 3 datasets\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "val_ds = val_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Confirm the number of batches in each\n",
    "print (f' There are {len(train_ds)} training batches.')\n",
    "print (f' There are {len(val_ds)} validation batches.')\n",
    "print (f' There are {len(test_ds)} testing batches.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498f8eb-bdaa-45bb-a809-f36014e03f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Repeat) display a sample single element \n",
    "example_X, example_y= train_ds.take(1).get_single_element()\n",
    "print(example_X,'\\n\\n',example_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d48f3a6-6cb5-4065-9c49-4508e1f390c4",
   "metadata": {},
   "source": [
    "A single element now contains 32 samples since we set  batch_size to 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401b00d-d9b2-416c-b171-2e3cace3dd92",
   "metadata": {},
   "source": [
    "### Create the Training Texts Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45da873-5e7a-4e92-b4ca-939ad5c0e1f2",
   "metadata": {},
   "source": [
    "> We need to get a version of our data that is **only the texts**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a346be-2bf5-483d-8c13-4074fb506730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the text_ds from ds_train\n",
    "text_ds = train_ds.map(lambda x,y: x)\n",
    "# Preview the text_ds\n",
    "text_ds.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785861e1-f9d0-40cf-b43c-32bbbf3ae147",
   "metadata": {},
   "source": [
    "# LECTURE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f4989-a040-4730-adfe-87321b0313e2",
   "metadata": {},
   "source": [
    "### Determine appropriate sequence length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9713fe-43cb-4bb9-aebc-e31601c543db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the length of the each text\n",
    "# We will split on each space, and then get the length\n",
    "df_ml['length (tokens)'] = df_ml['text'].map( lambda x: len(x.split(\" \")))\n",
    "df_ml['length (tokens)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473f73c-0ac0-461e-a186-bf45ad560e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 150\n",
    "ax = sns.histplot(data=df_ml, hue='rating', x='length (tokens)',kde=True,\n",
    "                stat='probability',common_norm=False)#, estimator='median',);\n",
    "ax.axvline(SEQUENCE_LENGTH, color='red', ls=\":\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1a448-8cc1-4bac-8896-0d278ebcfc5f",
   "metadata": {},
   "source": [
    "## Our First Deep Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95210b15-3546-48cb-9e2e-85669af45e6d",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46088607-41ee-49b1-93a4-d2405e591730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Create text Vectorization layer\n",
    "# SEQUENCE_LENGTH = 150\n",
    "EMBED_DIM = 100\n",
    "\n",
    "sequence_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "sequence_vectorizer.adapt(text_ds)\n",
    "VOCAB_SIZE = sequence_vectorizer.vocabulary_size()\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873c9d2-f3f2-4b75-9230-c4caef9ebb75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "rnn_model = Sequential([\n",
    "    sequence_vectorizer,\n",
    "    layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                              output_dim=EMBED_DIM, \n",
    "                              input_length=SEQUENCE_LENGTH)\n",
    "    ])\n",
    "\n",
    "# Add *new* layer\n",
    "rnn_model.add(layers.SimpleRNN(32))\n",
    "\n",
    "# Add output layer\n",
    "rnn_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model.compile(optimizer='adam',\n",
    "              loss='bce',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7e41c-89c9-4452-af8c-0888d4e605a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_callbacks(patience=5,#3,\n",
    "                  monitor='val_accuracy',\n",
    "                 restore_best_weights=False):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor,\n",
    "                                                 restore_best_weights=restore_best_weights)\n",
    "    return [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a31be-7344-4fc6-ab27-71439a895540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = rnn_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd54783-0891-4da0-aff9-0dbfd0cb3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    rnn_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd497e6f-a936-46c8-a175-920a5d729f98",
   "metadata": {},
   "source": [
    "> We will continue with this task and introduce and apply various sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38d6b5-ae58-4a3e-85d7-d5970e49b9a1",
   "metadata": {},
   "source": [
    "## Plan of Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb28733-618f-469a-9567-7e6a150013da",
   "metadata": {},
   "source": [
    ">**We will fit and evaluate several iterations of the following:**\n",
    ">- SimpleRNN:\n",
    "    - Change learning rate\n",
    "    - With recurrent_dropout\n",
    "    - Returning sequences and Averaging\n",
    "> - LSTM:\n",
    "    - Same as SimpleRNN\n",
    "> - GRU:\n",
    "    - Same as SimpleRNN\n",
    "\n",
    ">**We will also try:**\n",
    "> - Adding a Bidirectional layer\n",
    "> - Using GloVe Pretrained word embeddings\n",
    "> - Stacking multiple RNNS layers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f079de2-c928-42e1-a323-7d751854d94b",
   "metadata": {},
   "source": [
    "## Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721207e-992b-449f-8892-f053704a70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model(text_vectorization_layer):\n",
    "    \n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    # Add the SimpleRNN layer\n",
    "    model.add(layers.SimpleRNN(32))\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_callbacks(patience=5,\n",
    "                  monitor='val_accuracy',\n",
    "                  start_from_epoch=0,\n",
    "                 restore_best_weights=False):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor, start_from_epoch=start_from_epoch,\n",
    "                                                 restore_best_weights=restore_best_weights)\n",
    "    return [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931fc27-9106-48e2-b638-1bc8a9ef804f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create text Vectorization layer\n",
    "SEQUENCE_LENGTH = 150\n",
    "EMBED_DIM = 100\n",
    "\n",
    "sequence_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "sequence_vectorizer.adapt(text_ds)\n",
    "VOCAB_SIZE = sequence_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420f814-c361-4fad-af12-1175cafe8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and specify the vectorizer\n",
    "rnn_model = build_rnn_model(sequence_vectorizer)\n",
    "\n",
    "# Define number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = rnn_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks()\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74a307-abc8-4713-bc8f-44ca51062faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    rnn_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab6d86-3c5b-41f8-87a7-efef4947c663",
   "metadata": {},
   "source": [
    "> This model started overfitting very early. Let's add dropout and recurrent_dropout for the next model to prevent this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838dfd9e-45a2-4efa-85c7-56580bd1a00d",
   "metadata": {},
   "source": [
    "### Adding Recurrent Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119b25e-c091-410a-98bb-ff25656e302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    # Add recurrent_dropout=0.2 to the SimpleRNN \n",
    "    model.add(layers.SimpleRNN(32,recurrent_dropout=0.2))\n",
    "    \n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9d542-8435-4ffb-bad2-e77081bf179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and specify the vectorizer\n",
    "rnn_model = build_rnn_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = rnn_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145aa38b-78f6-47e2-9feb-0802eadc9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    rnn_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76afdb82-a66a-431e-b85e-60195bc3eeed",
   "metadata": {},
   "source": [
    "> For this particular task/model, adding dropout and recurrent dropout did not seem to help. We could easily spend much more time trying alternative amounts of dropout. We could also tune the number of units, the optimizer, the learning rate, etc.\n",
    "\n",
    "\n",
    "Next we will try returning the sequences and averaging them to see if this improves the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a0302-4662-455d-8032-1b3514addf60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T21:30:32.485298Z",
     "iopub.status.busy": "2024-01-10T21:30:32.484491Z",
     "iopub.status.idle": "2024-01-10T21:30:32.538247Z",
     "shell.execute_reply": "2024-01-10T21:30:32.537768Z",
     "shell.execute_reply.started": "2024-01-10T21:30:32.485259Z"
    }
   },
   "source": [
    "## SimpleRNN - Return_sequences = True & Global Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f115b-612a-479a-9bb4-a037a46c4a6c",
   "metadata": {},
   "source": [
    "### Demo: 🕹️ Intuition behind Global Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779fbf53-538f-43cf-8844-6645b789cb4d",
   "metadata": {},
   "source": [
    "Instead of returning a single final output, we can set the SimpleRNN to return the sequences.\n",
    "We need to transform/flatten the sequences before the final Dense layer. \n",
    "\n",
    "One way to do so is using the average vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec9a76-b3ae-4f0f-845b-c5e9c5ab7565",
   "metadata": {},
   "source": [
    "#### Saving the Previous Model's Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4fa0a-7880-4dc0-99ef-e06a2a807637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the embedding layer from the model and use .get_weights()\n",
    "embedding_weights=rnn_model.layers[1].get_weights()[0]\n",
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736cf3c-f693-4c7d-a973-c9a3269ee6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rows = vocab, col = embedding dimensions\n",
    "sequence_vectorizer.vocabulary_size(), EMBED_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e44a4-1c34-498e-950a-aaf24f5d399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the words and their corresponding vectors\n",
    "vector_dict = {}\n",
    "for i, word in enumerate(sequence_vectorizer.get_vocabulary()):\n",
    "    # Save the weights for word (based on numeric index)\n",
    "    vector_dict[word]= embedding_weights[i] \n",
    "len(vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949bf154-1cf7-42e8-803b-78abf6f0ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo vector for love\n",
    "vector_dict['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e00c1-c96e-4aa7-82b8-5daf2339371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm embed dim\n",
    "vector_dict['love'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f4099-a561-480c-8c65-fcf3d3cea24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm embed dim for vacuum\n",
    "# vector_dict['vaccum']\n",
    "vector_dict['vaccum'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd42c6-004d-4e86-b259-34e4b3133293",
   "metadata": {},
   "source": [
    "#### Using numpy to calcualte the average vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d0ee30-c912-4214-add6-a481edb7bdfa",
   "metadata": {},
   "source": [
    "Taking the maximum value of 3 vectors to get 1 vector with the mean/average vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77042bd-dec9-40da-8181-b60b3ed82173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking the word vectors into 1 array\n",
    "stacked_vectors  = np.vstack((vector_dict['hate'], vector_dict['love'] ,vector_dict['vacuum']))\n",
    "stacked_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3383ec-2e45-4ae6-96b6-9d68ef6934d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91887d00-697d-411a-aec4-a3581d1b71b4",
   "metadata": {},
   "source": [
    "> We now have 3 vectors combined into one matrix. Let's calculate the mean vector with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d43288-b4bb-4145-a6ad-cb7efe7b5bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Average values (relate to GlobalMaxPooling)\n",
    "avg_vector = np.mean(stacked_vectors,axis=0)\n",
    "avg_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818d789-0838-4d7d-8a96-07e045e814fa",
   "metadata": {},
   "source": [
    "> Let's confirm that the shape of the avg vector matches that of a single word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c376e-86a6-4ca9-846a-32212b40eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dict['love'].shape, avg_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7951d88-5f1c-4daf-86c6-e9131dfe8aad",
   "metadata": {},
   "source": [
    "#### Demonstrating the GlobalAveragePooling1D layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36574c-b09e-4ca1-a5a5-93202cef30e5",
   "metadata": {},
   "source": [
    "We can accomplish this averaging in the model using the GlobalAveragePooling1D layer. We will demonstrate how this works and prove that it is simply the average vector that we calculated with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a23db-cff8-43de-ab57-32ac547edf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model containing only GlobalAveragePooling\n",
    "average_layer = Sequential(layers.GlobalAveragePooling1D())\n",
    "\n",
    "# Run the stacked Vectors throught the average pooling layer\n",
    "try:\n",
    "    output = average_layer(stacked_vectors)\n",
    "    print(output.shape)\n",
    "except Exception as e:\n",
    "    display(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e40b64-82f5-4aa9-bc6e-d894dc586a0c",
   "metadata": {},
   "source": [
    "- The model expected a 3D tensor instead of a 2D one. We can add an empty extra dimension using `tf.expand_dims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ca9e5-b342-472e-a991-93595f2d944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third dimension to prevent errors\n",
    "stacked_vectors_tf = tf.expand_dims(stacked_vectors,0)\n",
    "stacked_vectors_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49db837-28ff-4070-94ca-2e1f340cf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the new tensor with the expanded dims\n",
    "stacked_vectors_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544376e2-d2ce-4d47-afa5-452ef8771693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model containing only GlobalAveragePooling\n",
    "average_layer = Sequential(layers.GlobalAveragePooling1D())\n",
    "\n",
    "# Run the stacked Vectors throught the average pooling layer\n",
    "output = average_layer(stacked_vectors_tf)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfa2cb-7514-41cc-a7b7-48292ed5ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the output from the global pooling layer (as a numpy array)\n",
    "output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7848a-0f3b-4c55-bd2e-51dfcf75bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the GlobalAverage Pooling layer returns same result as the avg_vector\n",
    "(output.numpy() == avg_vector)#.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ffa26-a5e9-4c19-a85e-c2d4de2e9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the GlobalAverage Pooling layer returns same result\n",
    "np.isclose(output.numpy(), avg_vector).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d0cec-8545-4038-8726-a2da2d89e525",
   "metadata": {},
   "source": [
    "> Now let's add this to the model and see how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf40f0-dffa-4a39-810d-05cf0d31e1a5",
   "metadata": {},
   "source": [
    "### Using GlobalAveragePooling1D in Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413be8a1-1205-4c50-acf4-37d338f94d87",
   "metadata": {},
   "source": [
    "- We must set return_sequences = True  for the RNN layer, and then add the GlobalAveragePooling1D layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b567b9-f7d7-4dd6-989c-e538d96da6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model_pool(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "\n",
    "    \n",
    "    ## Add A SimpleRNN layer that will return sequences\n",
    "    model.add(layers.SimpleRNN(32,return_sequences=True))\n",
    "    \n",
    "    \n",
    "    ## Add a global average pooling 1d layer \n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "              \n",
    "\n",
    "    \n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144adeed-91fc-4719-97c7-3022d5edffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "rnn_model_pool = build_rnn_model_pool(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = rnn_model_pool.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c680a-ffd2-4f21-b909-20326b420952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    rnn_model_pool, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7315bd-19f5-48d0-82d6-6e72dde1b338",
   "metadata": {},
   "source": [
    "> Using the average vector helped our model a lot! Notice how high the recall score is for the 1 class.  However, its still overfitting so lets try adding recurrent dropout and dropout regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9fc80-8a51-4faf-ae03-ea824417270d",
   "metadata": {},
   "source": [
    "###  Using Recurrent Dropout + return_sequences=True and Global Pooling layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac1659-bfb9-41e1-8489-effec0fd7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model_pool(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    ## Add recurrent_dropout = 0.2 and dropout=0.2 to prevous SimpleRNN\n",
    "    model.add(layers.SimpleRNN(32,recurrent_dropout=.2, dropout=.2,return_sequences=True))\n",
    "\n",
    "    \n",
    "    ## Add a global average pooling 1d layer \n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "              \n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad05b8-73e6-4f5f-9a89-4b02d3a7de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the lstm model and specify the vectorizer\n",
    "rnn_model_pool = build_rnn_model_pool(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = rnn_model_pool.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d5d469-4fc6-4ecb-aea7-cf5d30b106be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    rnn_model_pool, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4cb3f-ea75-45ba-a999-da86070dfda3",
   "metadata": {},
   "source": [
    "> Adding dropout and recurrent dropout helped our model with the 1 class. While have a lower accuracy, we have better recall for the 1 class.\n",
    "\n",
    "There's a lot more we could tune, but let's move onto the next sequence model that was developed: the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422cc80-004d-4423-a838-f8a1e628e1b6",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f64f19-1f7c-4a2e-bd0a-f0ab58bcf441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    ## Add an LSTM layer with 32 units\n",
    "    model.add(layers.LSTM(32))\n",
    "    \n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762efb8-3f01-4bf0-aefb-057f203593e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "lstm_model = build_lstm_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = lstm_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1bb65-bc1d-4361-8a29-4a809d5a29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    lstm_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb10d2-7a57-4621-bdd2-ac194b36b162",
   "metadata": {},
   "source": [
    "> the baseline LSTM is already outperforming our baseline SimpleRNN model.  Let's try using the average word vectors again, since overfitting isn't as much of an issue vs. our earlier SimpleRNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5f1ec-87a0-4d2d-800f-53c7a2d43a04",
   "metadata": {},
   "source": [
    "###  Using return_sequences=True and Global Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3518c-86b9-48ae-945a-bb8e21d1db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "\n",
    "\n",
    "    # Add an LSTM layer that will return sequences\n",
    "    model.add(layers.LSTM(32,return_sequences=True))\n",
    "    \n",
    "    ## Add a global average pooling 1d layer \n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    \n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc71d0-e531-4f9a-8805-fbf12e90eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the lstm model and specify the vectorizer\n",
    "lstm_model = build_lstm_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = lstm_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f88c2-d8c4-4ce9-b3fb-fbf5ce75b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    lstm_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a1ba7-35d0-4e3e-842f-c5de3154e078",
   "metadata": {},
   "source": [
    "> Using the average sequence has helped our LSTM model as well. We have better accuracy and recall scores for both classes. The model started to overfit after epoch 4, so let's add recurrent dropout and dropout again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815036d1-0a4d-47a9-87fc-0a271c97ef34",
   "metadata": {},
   "source": [
    "### Add Dropout/Recurrent Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a97aa-198d-49dd-8e94-b38d14c3b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    # Add recurrent_dropout = 0.1 and dropout=0.1 to prevous LSTM\n",
    "    \n",
    "    \n",
    "    ## Add a global average pooling 1d layer \n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0818258-0e1f-4df3-8de2-87ea08efcc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "lstm_model = build_lstm_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = lstm_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09025a7b-e041-4a02-a87b-ff0e05111add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    lstm_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e84de9-8655-4226-b45d-985bed149697",
   "metadata": {},
   "source": [
    "> Adding dropout and recurrent_dropout has helped our model again. There are still other parameters we would try, but let's move onto the next sequence model that was developed: the GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9c258-796c-433c-999e-45b33691da2f",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1271a33-f0bf-4629-a47c-49836013411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    # Add a GRU layer with 32 units\n",
    "    model.add(layers.GRU(32)) \n",
    "\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4a156-a279-495f-ba6c-9e9c9359288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "gru_model = build_gru_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = gru_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history, figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f87b6-281e-4bbc-9a02-0e0d09cb9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    gru_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3555ebc-8ef8-494c-a73f-8f93a0ad5789",
   "metadata": {},
   "source": [
    "> The basline GRU is peforming much worse than the baseline LSTM. The accuracy and val_accuracy stayed pretty flat for much of the training epochs and the model is very biased towards predicting the 0 class. Our current GRU model may not be complex enough.\n",
    "> We can try increasing the number of GRU units and a faster learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab246de4-6b60-4647-8053-6a23965d36bd",
   "metadata": {},
   "source": [
    "### GRU + More Units + Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61363dc7-a42e-4be3-9d51-173d67b8d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    # Add a GRU layer with 64 units \n",
    "    model.add(layers.GRU(64,recurrent_dropout=.2, dropout=.2))#, recurrent_dropout=.2, dropout=.1))\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .01), ## Increase  learning rate to .01\n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486cff7-2070-4d1f-868f-3b429b0b8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "gru_model = build_gru_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 1\n",
    "# Fit the model\n",
    "history = gru_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history, figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f08a4e6-8fab-4d30-b10d-63262c487a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    gru_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0064b-aa65-405e-8db1-c3120faa947d",
   "metadata": {},
   "source": [
    "> The increased complexity (nunits) plus faster learning rate helped dramatically! The model is overfitting so let's add dropout as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67222823-b0b0-463b-b8db-cad280f5ae84",
   "metadata": {},
   "source": [
    "### Add Dropout/Recurrent Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12347499-deb8-4d75-ba07-5081783ce78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    # Add recurrent_dropout = 0.2 and dropout=0.2 to prevous GRU\n",
    "\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .01), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27682cb-0d97-4848-a1a1-d09725d68141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "gru_model = build_gru_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = gru_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187aec7c-3528-4086-9cf6-b0e85c5ef04c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    gru_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d7921d-6dbc-4713-a0c7-88d9f98da15b",
   "metadata": {},
   "source": [
    "> This GRU is struggling with our current task. This may be due to insufficient complexity. One way we can increaser the complexity is to combine 2 GRU layers stacked together. Using multiple layers with fewer units is generally preferable to increasre the number of units in a single layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25254dce-d1e4-45ed-91a1-020a7d58df41",
   "metadata": {},
   "source": [
    "## Stacked GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90787c-e118-4569-9445-fa3db5e4b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stacked_gru_model(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    ## Add GRU layer with 64 units, recurrent dropout =0.1 , and return sequences\n",
    "\n",
    "    \n",
    "    ## Add a dropout layer set to 0.2\n",
    "\n",
    "    \n",
    "    ## Add GRU layer with 64 units, recurrent dropout =0.1 , and do NOT return sequences\n",
    "    \n",
    "\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .01), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3309b9f-4211-49c1-8170-a994aafb3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "stacked_gru_model = build_stacked_gru_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = stacked_gru_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22845f8-41a4-4ea7-a7c1-35f0920ed0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    stacked_gru_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d1f8f-9ed5-4b87-8dca-8228a09f2881",
   "metadata": {},
   "source": [
    "### Using Return Sequences = True + GlobalAveragePooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4e694-4ba5-4b04-8cf5-f98bbd7d1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stacked_gru_model(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    # Add GRU layer with 64 units, recurrent dropout =0.1 , and return sequences\n",
    "\n",
    "    \n",
    "    # Add a dropout layer set to 0.2\n",
    "\n",
    "    \n",
    "    # Add GRU layer with 64 units, recurrent dropout =0.1 , and  return sequences\n",
    "\n",
    "\n",
    "    \n",
    "    ## Add the global pooling 1D layer\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    \n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .01), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e45d2-ccf0-40cc-bc43-d1137da8dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "stacked_gru_model = build_stacked_gru_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = stacked_gru_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f03c5-9d71-4a7b-8cbd-20cfb3fc822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    stacked_gru_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f6d02-260a-4a16-8891-b960766ae6c4",
   "metadata": {},
   "source": [
    "> The stacked GRU with the global pooling layer improved vs. the previous model.\n",
    "> There is more we could continue to tune, of course, (nunits, dropout, optimizer, etc.)\n",
    "> An extension of using a stacked model is using a Bidrectional layer, which will make a duplicate of the layer inside it.\n",
    "> The second layer will receive the sequences in the reverse order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2234c7ff-93db-4a86-a1cd-4ad148ca6189",
   "metadata": {},
   "source": [
    "## Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9dff5b-9688-4fae-a5ea-38f7038f24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bi_gru_model(text_vectorization_layer):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    ## Add a Bidirectional layer wrapped around a GRU layer with:\n",
    "    ## 64 units,recurrent_dropout=0.2, dropout=0,2\n",
    "\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .01), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc9056-6b91-4266-895e-f6f118e423fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "bi_gru_model = build_bi_gru_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = bi_gru_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50838a-aac4-408f-bb75-4e10c7bde27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    bi_gru_model, X_train=train_ds, \n",
    "    X_test=test_ds, # history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d109ba-860c-4ac5-b15b-3824700b1d70",
   "metadata": {},
   "source": [
    "> We could also attempt using the average vectors by adding the GlobalAveragePooling layer.\n",
    ">\n",
    "> \n",
    "> We could continue to explore bidirectional models and revisit SimpleRNNs and LSTMs. Bidirectional Layers can be used with any of the sequence models we've covered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b37c47-bb28-4501-a7a1-5b7d95b63001",
   "metadata": {},
   "source": [
    "# 🕹️ Now Your Turn: Train A Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d786d06-e7a4-47f7-b68b-906260572960",
   "metadata": {},
   "source": [
    "> In a breakout room:\n",
    "    > - **Add the missing sequence layer(s) to the model below.**\n",
    "    > - Apply some of the additonal techniques we've shown thus far. \n",
    "    > - **Discuss you your breakout room group and decide which model to try.**\n",
    "    > - For example, try differnet sequence layers, try different values for dropout, number of units, using a GlobalAveragePooling layer or not, using a bidirectional layer or not, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52354e53-9d25-4a72-928a-0e1011c28020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_your_model(text_vectorization_layer):\n",
    "    \n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "    #### Add your sequence layer and whatever additonal parameters/techniques you want to try\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96718d1-c4de-4af2-9c7d-34b396d9ed35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Build the lstm model and specify the vectorizer\n",
    "your_rnn_model = build_your_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = your_rnn_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16045a49-b75f-4e94-ba67-46e1d6ac177b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    your_rnn_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da2f15-085e-4a99-b5e8-18acf6256dd5",
   "metadata": {},
   "source": [
    "## 📚Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d22e11a-843f-4ac3-8d94-60f9dba16d8d",
   "metadata": {},
   "source": [
    "###  Using GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2336a6-c7f8-4ff2-97cd-73517e59a91a",
   "metadata": {},
   "source": [
    "- [Click here](https://nlp.stanford.edu/data/glove.6B.zip) to start donwnloading GloVe zip file (glove.6B.zip)\n",
    "- Unzip the downloaded zip archive.\n",
    "- Open the extracted folder and find the the `glove.6B.100d.txt` file. (Size is over 300MB )\n",
    "- Move the text file from Downloads to the same folder as this notebook.\n",
    "- **Make sure to ignore the large file using GitHub Desktop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d787ab9-41a7-4c09-bd99-76722f369f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# Load GloVe vectors into a gensim model\n",
    "#glove_model = KeyedVectors.load_word2vec_format(\"../glove.6B.100d.txt\", binary=False, no_header=True)\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"../Lecture 1/glove.6B.100d.txt\", binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859c359-c117-4d4c-b745-a5c0591c8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now use `glove_model` to access individual word vectors, similar to a dictionary\n",
    "vector = glove_model['king']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2cfa48-3101-4f11-b95f-86a2da36dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f16f619-2d0b-4443-869a-801a3fb40313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similarity between words\n",
    "glove_model.similarity('king', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0366fee-7698-44a3-b281-1d55518ec2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform word math\n",
    "result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6bd42-f0a0-4ad0-a301-5790f22b24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use glove to calculate the most similar\n",
    "glove_model.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2631163a-290a-4086-8525-2ead60ae42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculating new vector for word math\n",
    "new_vector = glove_model['king'] - glove_model['man'] + glove_model['woman']\n",
    "new_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62f9b9-696b-4186-baa0-96306023a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .most_similar with an array\n",
    "glove_model.most_similar(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446cc69-1265-4e2e-a777-875ee070fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculating new vector for word math\n",
    "new_vector = glove_model['monarchy'] + glove_model['vote'] + glove_model['government']\n",
    "glove_model.most_similar(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf957d-31af-4be4-aeeb-dc909a6a2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculating new vector for word math\n",
    "new_vector = glove_model['baby'] + glove_model['age']\n",
    "glove_model.most_similar(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede594e-cd14-4cdd-85ad-19065d899744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Manually calculating new vector for word math\n",
    "new_vector = glove_model['baby'] + glove_model['baby']\n",
    "glove_model.most_similar(new_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682d1d0-2155-4ff6-b7aa-96e5cc49e52a",
   "metadata": {},
   "source": [
    "### Using Pre-trained Vectors in The Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0625f75-0421-41c5-859b-4968e1f7a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocab size for creating embedding matrix \n",
    "VOCAB_SIZE = sequence_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd9a78-0f7c-4962-9e2d-660ffc329183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty matrix the same shape as our EMbedding layer \n",
    "# to hold the word vectors from GloVe\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBED_DIM))\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad165c4-7688-4c3d-af1c-7a5400d17674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Create an empty list to save words that were not found in GloVe\n",
    "unmatched_vocab = []\n",
    "\n",
    "# (Required) Loop to get each word from vocabulary and get its pre-trained vector from GloVe model\n",
    "for i, word in enumerate(sequence_vectorizer.get_vocabulary()):\n",
    "    try:\n",
    "        embedding_matrix[i] = glove_model[word]\n",
    "    except KeyError:\n",
    "        # (Optional) If the word is not in the GloVe vocabulary,save it\n",
    "        unmatched_vocab.append(word)\n",
    "        pass\n",
    "        \n",
    "\n",
    "## (Optional) Printing How many Tokens were Unmatched\n",
    "percent_unmatched = len(unmatched_vocab)/VOCAB_SIZE*100\n",
    "print(f\"- {len(unmatched_vocab)} ({percent_unmatched:.2f}%) tokens not found in glove embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7380121f-5d48-4fb3-ac83-ab11e655da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying example of unmatched tokens\n",
    "unmatched_vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7546598-7ce6-4e12-8091-52e73a68b4e3",
   "metadata": {},
   "source": [
    ">- Now we can use this new embedding matrix as the initial weights for an Embedding layer.\n",
    ">- By setting the emedding layer trainable=False, we will freeze the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93936b7b-ec8b-4400-a20b-565bb974011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding layer using the embedding matrix\n",
    "\n",
    "# First, make a Constant initializer with the embedding matrix\n",
    "initializer = tf.keras.initializers.Constant(embedding_matrix)\n",
    "\n",
    "# Then use this initializer as the embedding_initializer argument.\n",
    "glove_embedding_layer = layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                            output_dim=EMBED_DIM,\n",
    "                            # Use the initializer as embeddings_initializer\n",
    "                            embeddings_initializer =initializer,\n",
    "                            # Keeps the embeddings fixed\n",
    "                            trainable=False,  \n",
    "                            input_length=SEQUENCE_LENGTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4157b-d493-421c-9b7a-1d2ea83dfbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model_glove(text_vectorization_layer, embedding_matrix):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "\n",
    "    ## Add the initialzier with the embedding matrix weights \n",
    "    # and add as the  glove_embedding_layer  (Copy Example Above)\n",
    "    initializer = tf.keras.initializers.Constant(embedding_matrix)\n",
    "\n",
    "    # Then use this initializer as the embedding_initializer argument.\n",
    "    glove_embedding_layer = layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                            output_dim=EMBED_DIM,\n",
    "                            # Use the initializer as embeddings_initializer\n",
    "                            embeddings_initializer =initializer,\n",
    "                            # Keeps the embeddings fixed\n",
    "                            trainable=False,  \n",
    "                            input_length=SEQUENCE_LENGTH)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and glove embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        glove_embedding_layer\n",
    "        ])\n",
    "\n",
    "    \n",
    "    # Add GRU layer with 64 units, recurrent dropout=.1 droput.1, and return sequences\n",
    "    model.add(layers.GRU(64, recurrent_dropout=.1,dropout=.1, return_sequences=True))\n",
    "    # Add a Global Pooling layer\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2d821-f55f-468d-8101-375db242382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "gru_glove_model = build_gru_model_glove(sequence_vectorizer, embedding_matrix)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = gru_glove_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e69e8-4bba-4b0e-91b4-986817d6af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    gru_glove_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fd02b-cbbe-4328-93ab-b278f18415e3",
   "metadata": {},
   "source": [
    ">The models performance did not increase dramatically. We prevented the model from updating the word embeddings from GloVe, but we can also allow then model to update them as it trains the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cbe296-27cb-4d5d-a752-9d5de6c4c005",
   "metadata": {},
   "source": [
    "### Pretrained Embedding - Trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084cdc8b-bb0b-4edc-b60d-ffd5f4169947",
   "metadata": {},
   "source": [
    ">  This time we will allow the model to update the values of the embedding. There were over 600 tokens that we did not have a glove vector for, so the model used all 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2719b814-d023-4de9-998d-33fef4773d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model_train_glove(text_vectorization_layer, embedding_matrix):\n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    ## Copy the initializer and glove_embedding layer from previous model\n",
    "    ## BUT Change trainable to True (instead of False)\n",
    "    # Create the embedding layer using the embedding matrix\n",
    "    initializer = tf.keras.initializers.Constant(embedding_matrix)\n",
    "\n",
    "    # Then use this initializer as the embedding_initializer argument.\n",
    "    glove_embedding_layer = layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                            output_dim=EMBED_DIM,\n",
    "                            # Use the initializer as embeddings_initializer\n",
    "                            embeddings_initializer =initializer,\n",
    "                            # Keeps the embeddings fixed\n",
    "                            trainable=True,  \n",
    "                            input_length=SEQUENCE_LENGTH)\n",
    "\n",
    "    \n",
    "\n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        glove_embedding_layer\n",
    "        ])\n",
    "\n",
    "    # Add GRU layer with 64 units, recurrent dropout=.1 droput.1, and return sequences\n",
    "    model.add(layers.GRU(64, recurrent_dropout=.1,dropout=.1, return_sequences=True))\n",
    "    # Add a Global Pooling layer\n",
    "    model.add(layers.GlobalAveragePooling1D())\n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e762c-aea3-40e1-8779-8766cfe7c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "gru_glove_model = build_gru_model_train_glove(sequence_vectorizer, embedding_matrix)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = gru_glove_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a51264-2c5c-4907-9a78-fab19df428d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    gru_glove_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee22cb-690a-41d3-9e7c-1f733fe2eb7c",
   "metadata": {},
   "source": [
    "> This is a very promising start! We would want to continue testing using other techniques from earlier in this activity here as well (LSTM, bidrectional, stacked,etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e520c6-3ff5-477c-8f24-01e15e0d0f33",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f1c71-51fc-40f3-90b8-8c30486eb425",
   "metadata": {},
   "source": [
    "- There are many tactics that we can leverage when working with Sequence models.  Ultimately, the best params will vary with the dataset and the text preprocessing options.\n",
    "- We can also tune all of the standard parameters from previous lessons on Deep learning (e.g. number of units, activation function,etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12dbbd9-60d5-4e74-a4cd-8acf0bff3890",
   "metadata": {},
   "source": [
    "# 🕹️(Optional) Your Turn: Try Alternative Text Preprocessing Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d54925-a7a3-400f-98f2-68062b6709de",
   "metadata": {},
   "source": [
    "Everything we have tuned/tested thus far has been related to the model itself. There are still several aspects of the text vectorization that we could also try:\n",
    "- EMBED_DIM (50,100,200,etc. )\n",
    "- SEQUENCE_LENGTH (50,100,150)\n",
    "- standardization (text vectorizer)\n",
    "- ngrams (text vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ae4b0-6fd0-4b2f-bf08-e33108229ae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create text Vectorization layer\n",
    "SEQUENCE_LENGTH = None\n",
    "EMBED_DIM = None\n",
    "\n",
    "sequence_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=None\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "sequence_vectorizer.adapt(text_ds)\n",
    "VOCAB_SIZE = sequence_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a73ec7-be39-454c-882c-9b79e2ed1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_your_model(text_vectorization_layer):\n",
    "    \n",
    "    # Getting Text Parameters from TextVectorization Layer\n",
    "    VOCAB_SIZE = text_vectorization_layer.vocabulary_size()\n",
    "    SEQUENCE_LENGTH = text_vectorization_layer.get_config()['output_sequence_length']\n",
    "    \n",
    "    \n",
    "    # Define sequential model with pre-trained vectorization layer and *new* embedding layer\n",
    "    model = Sequential([\n",
    "        text_vectorization_layer,\n",
    "        layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                  output_dim=EMBED_DIM, \n",
    "                                  input_length=SEQUENCE_LENGTH)\n",
    "        ])\n",
    "        \n",
    "      \n",
    "    #### Add your sequence layer(s) and whatever additonal parameters/techniques you want to try\n",
    "\n",
    "    \n",
    "    \n",
    "    # Add output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    " \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizers.legacy.Adam(learning_rate = .001), \n",
    "                  loss='bce',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def get_callbacks(patience=5,\n",
    "                  monitor='val_accuracy',\n",
    "                  start_from_epoch=0,\n",
    "                 restore_best_weights=False):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(patience=patience, monitor=monitor, start_from_epoch=start_from_epoch,\n",
    "                                                 restore_best_weights=restore_best_weights)\n",
    "    return [early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff158f-216c-4694-b2de-64e488913eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lstm model and specify the vectorizer\n",
    "your_model = build_your_model(sequence_vectorizer)\n",
    "\n",
    "# Defien number of epocs\n",
    "EPOCHS = 30\n",
    "# Fit the model\n",
    "history = your_model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=get_callbacks(patience=5)\n",
    ")\n",
    "fn.plot_history(history,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a783c-8a02-462b-b568-33ed6fae52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results\n",
    "results = fn.evaluate_classification_network(\n",
    "    your_model, X_train=train_ds, \n",
    "    X_test=test_ds,# history=history\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be2ddf1-f799-42df-ace3-e0dfaa395b18",
   "metadata": {},
   "source": [
    "> **Did you find any text preprocessing options that significantly improved the model?** If so, share with the class in the main zoom room or on Discord!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
